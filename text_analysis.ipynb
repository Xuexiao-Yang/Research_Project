{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"patientTL.pickle\",'rb') as handle:\n",
    "    tweets = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print len(tweets)\n",
    "print tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "user_tweets = defaultdict(list)\n",
    "for tweet in tweets:\n",
    "    text =  tweet[1]\n",
    "    user_tweets[tweet[2]].append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hashtag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "list_with_tags = []\n",
    "for i in range (0,len(tweets)):\n",
    "    temp_list = re.findall(r'^#[a-z]{8,}(?=\\s)|(?<=\\s)#[a-z]{8,}(?=\\s)|(?<=\\s)#[a-z]{8,}$',tweets[i][1])\n",
    "    if temp_list != []:\n",
    "        list_with_tags.extend(temp_list)\n",
    "\n",
    "collected_list = []\n",
    "for j in range (0,len(list_with_tags)):\n",
    "    temp_list = re.sub('#','',list_with_tags[j])\n",
    "    collected_list.append(temp_list)\n",
    "\n",
    "print \"The number of hashtages I have collected is:\"\n",
    "print len(collected_list)\n",
    "print collected_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "\n",
    "dictionary = set(words.words()) # words is a Python list\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def BackMaxMatch(strings):\n",
    "    token = []\n",
    "    pos_front = 0\n",
    "    pos_end = len(strings)\n",
    "    while len(strings) > 0:\n",
    "        word = word_lemmatizer.lemmatize(strings[pos_front:pos_end])\n",
    "        if word in dictionary:\n",
    "            token.append(strings[pos_front:pos_end])\n",
    "            strings = strings[0:pos_front]\n",
    "            pos_end = len(strings)\n",
    "            pos_front = 0\n",
    "        else:\n",
    "            if len(strings[pos_front:pos_end]) == 1:\n",
    "                token.append(strings[pos_front:pos_end])\n",
    "                strings = strings[0:pos_front]\n",
    "                pos_end = len(strings)\n",
    "                pos_front = 0\n",
    "            else:\n",
    "                pos_front = pos_front + 1\n",
    "    token.reverse()\n",
    "    return token\n",
    "\n",
    "token_list_BMM = []\n",
    "for i in range (0,len(collected_list)):\n",
    "    temp_list = BackMaxMatch(collected_list[i])\n",
    "    token_list_BMM.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ForwardMaxMatch(strings):\n",
    "    token = []\n",
    "    pos_end = len(strings)\n",
    "    while len(strings) > 0:\n",
    "        word = word_lemmatizer.lemmatize(strings[:pos_end])\n",
    "        if word in dictionary:\n",
    "            token.append(strings[:pos_end])\n",
    "            strings = strings[pos_end:]\n",
    "            pos_end = len(strings)\n",
    "        else:\n",
    "            if len(strings[:pos_end]) == 1:\n",
    "                token.append(strings[:pos_end])\n",
    "                strings = strings[pos_end:]\n",
    "                pos_end = len(strings)\n",
    "            else:\n",
    "                pos_end = pos_end - 1\n",
    "    return token\n",
    "\n",
    "\n",
    "token_list_FMM = []\n",
    "for i in range (0,len(collected_list)):\n",
    "    temp_list = ForwardMaxMatch(collected_list[i])\n",
    "    token_list_FMM.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"The different segmentations of hashtages by the BackMaxMatch(BMM) and ForwadMaxMatch(FMM) is:\"\n",
    "\n",
    "for j in range (0,len(collected_list)):\n",
    "    if cmp(token_list_BMM[j],token_list_FMM[j]) != 0:\n",
    "        print \"Position of hashtage: \" + str(j+1)\n",
    "        print \"#\" + str(collected_list[j])\n",
    "        print \" The BMM result is: \" + str(token_list_BMM[j])\n",
    "        print \" The FMM result is: \" + str(token_list_FMM[j])\n",
    "        \n",
    "        if len(token_list_BMM[j]) <= len(token_list_FMM[j]):\n",
    "            print \" The best result is: \" + str(token_list_BMM[j]) + '\\n'\n",
    "        else:\n",
    "            print \" The best result is: \" + str(token_list_FMM[j]) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Optimize the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "positive_tweets = nltk.corpus.twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "negative_tweets = nltk.corpus.twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "\n",
    "def text_cleaner(tweets):\n",
    "    cleaned_token = []\n",
    "    for i in range (0,len(tweets)):\n",
    "        single_token = []\n",
    "        for j in range(0,len(tweets[i])):\n",
    "            if tweets[i][j] not in stop_words and tweets[i][j].isalpha():\n",
    "                temp = tweets[i][j]\n",
    "                if len(temp) != 0:\n",
    "                    single_token.append(temp)\n",
    "        cleaned_token.append(single_token)\n",
    "    return cleaned_token\n",
    "\n",
    "positive = text_cleaner(positive_tweets)\n",
    "negative = text_cleaner(negative_tweets)\n",
    "\n",
    "#First split the dataset as the 80% of train, 20% of rest. Then split the rest part into half develop and half test.\n",
    "pos_train, pos_rest, neg_train, neg_rest = train_test_split(positive, negative, train_size = 0.8)\n",
    "pos_develop, pos_test, neg_develop, neg_test = train_test_split(pos_rest, neg_rest, train_size = 0.5)\n",
    "\n",
    "pos_train.extend(neg_train)\n",
    "pos_develop.extend(neg_develop)\n",
    "pos_test.extend(neg_test)\n",
    "\n",
    "train_data = pos_train\n",
    "develop_data = pos_develop\n",
    "test_data = pos_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "# use the entire dataset to bulid a unified vector, this could ensure the subset feature matrix\n",
    "# transformed in to same dimension for continues processing\n",
    "vectorizer = DictVectorizer()\n",
    "entire_dataset = train_data + develop_data + test_data  \n",
    "\n",
    "vector_shape_unified = []\n",
    "for i in range (0,len(entire_dataset)):\n",
    "    temp = get_BOW(entire_dataset[i])\n",
    "    vector_shape_unified.append(temp)\n",
    "unified_set = vectorizer.fit_transform(vector_shape_unified)\n",
    "\n",
    "def prepare_data(vectorizer,data,feature_extractor):\n",
    "    feature_matrix = []\n",
    "    label = []\n",
    "    for j in range (0,len(data)):\n",
    "        feature_dict = feature_extractor(data[j])\n",
    "        feature_matrix.append(feature_dict)\n",
    "        if j < (len(data)/2):\n",
    "            label.append(\"positive\")\n",
    "        else:\n",
    "            label.append(\"negative\")\n",
    "    dataset = vectorizer.transform(feature_matrix)\n",
    "    return dataset,label\n",
    "\n",
    "train_set,train_label = prepare_data(vectorizer,train_data,get_BOW)\n",
    "develop_set,develop_label = prepare_data(vectorizer,develop_data,get_BOW)\n",
    "test_set,test_label = prepare_data(vectorizer,test_data,get_BOW)\n",
    "\n",
    "\n",
    "def prepare_data_test(vectorizer,data,feature_extractor):\n",
    "    feature_matrix = []\n",
    "    for j in range (0,len(data)):\n",
    "        feature_dict = feature_extractor(data[j])\n",
    "        feature_matrix.append(feature_dict)\n",
    "    dataset = vectorizer.transform(feature_matrix)\n",
    "    return dataset\n",
    "\n",
    "test_set = prepare_data_test(vectorizer,test_data,get_BOW)\n",
    "\n",
    "def NB_parameter_optimize(train,t_label,develop,d_label):\n",
    "    print \"In Naive Bayes Classifier we adjust the Laplace/Lidstone smoothing parameter (alpha):\"\n",
    "    temp_accuracy = 0\n",
    "    val = 0.1\n",
    "    while (val <= 3):\n",
    "        NB_classifier = MultinomialNB(alpha = val, class_prior=None, fit_prior=True)\n",
    "        NB_classifier.fit(train_set,train_label)\n",
    "        predicted = NB_classifier.predict(develop_set)\n",
    "        accuracy = accuracy_score(develop_label,predicted)\n",
    "        print \"   The alpha is \" + str(val) + \", the accuracy is \" + str(accuracy) + \".\"\n",
    "        if temp_accuracy <= accuracy:\n",
    "            optimal_NB_parameter = val\n",
    "            temp_accuracy = accuracy \n",
    "        val = val + 0.1\n",
    "    return optimal_NB_parameter\n",
    "\n",
    "NB_best_parameter = NB_parameter_optimize(train_set,train_label,develop_set,develop_label)\n",
    "print \"The optimal value of the parameter for Naive-Bayes classifier is: \" + str(NB_best_parameter) +'\\n'\n",
    "\n",
    "\n",
    "def LR_optimize(train,t_label,develop,d_label):\n",
    "    print \"In Logistic Regression classifier we adjust the regularization strength parameter (C):\"\n",
    "    temp_accuracy = 0\n",
    "    val = 0.1\n",
    "    while (val <= 3):\n",
    "        LR_classifier = LogisticRegression(C = val)\n",
    "        LR_classifier.fit(train_set,train_label)\n",
    "        predicted = LR_classifier.predict(develop_set)\n",
    "        accuracy = accuracy_score(develop_label,predicted)\n",
    "        print \"   The C is \" + str(val) + \", the accuracy is \" + str(accuracy) + \".\"\n",
    "        if temp_accuracy <= accuracy:\n",
    "            optimal_LR_parameter = val\n",
    "            temp_accuracy = accuracy \n",
    "        val = val + 0.1\n",
    "    return optimal_LR_parameter\n",
    "\n",
    "LR_best_parameter = LR_optimize(train_set,train_label,develop_set,develop_label)\n",
    "print \"The optimal value of the parameter for Logistic Regression classifier is: \" + str(LR_best_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NB_classifier = MultinomialNB(alpha = NB_best_parameter, class_prior=None, fit_prior=True)\n",
    "NB_classifier.fit(train_set,train_label)\n",
    "NB_predicted = NB_classifier.predict(test_set)\n",
    "NB_accuracy = accuracy_score(test_label,NB_predicted)\n",
    "print \"The accuracy of Naive-Bayes model is: \" + str(NB_accuracy)\n",
    "print \"and the macroaveraged f-score is: \" + str(f1_score(test_label, NB_predicted, average='macro')) + '\\n'\n",
    "\n",
    "LR_classifier = LogisticRegression(C = LR_best_parameter)\n",
    "LR_classifier.fit(train_set,train_label)\n",
    "LR_predicted = LR_classifier.predict(test_set)\n",
    "LR_accuracy = accuracy_score(test_label,LR_predicted)\n",
    "print \"The accuracy of Logistic Regression model is: \" + str(LR_accuracy)\n",
    "print \"and the macroaveraged f-score is: \" + str(f1_score(test_label, LR_predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_lemmatizer = WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "temp_stop_words = stopwords.words('english')\n",
    "temp_stop_words.append(u'https')\n",
    "temp_stop_words.append(u'co')\n",
    "stop_words = set(temp_stop_words)\n",
    "\n",
    "\n",
    "def text_tokenizer(tweets):\n",
    "    tokenized_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tokenized_tweet = word_tokenizer.tokenize(tweet)\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "    return tokenized_tweets\n",
    "\n",
    "\n",
    "def text_cleaner(tweets):\n",
    "    cleaned_token = []\n",
    "    for i in range (0,len(tweets)):\n",
    "        single_token = []\n",
    "        for j in range(0,len(tweets[i])):\n",
    "            word = word_lemmatizer.lemmatize(tweets[i][j])\n",
    "            if word not in stop_words and tweets[i][j].isalpha():\n",
    "                temp = tweets[i][j]\n",
    "                if len(temp) != 0:\n",
    "                    single_token.append(temp)\n",
    "        cleaned_token.append(single_token)\n",
    "    return cleaned_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis(test_tweets,best_NB, best_logistic):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    positive_tweets = nltk.corpus.twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "    negative_tweets = nltk.corpus.twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "\n",
    "    def text_cleaner(tweets):\n",
    "        cleaned_token = []\n",
    "        for i in range (0,len(tweets)):\n",
    "            single_token = []\n",
    "            for j in range(0,len(tweets[i])):\n",
    "                if tweets[i][j] not in stop_words and tweets[i][j].isalpha():\n",
    "                    temp = tweets[i][j]\n",
    "                    if len(temp) != 0:\n",
    "                        single_token.append(temp)\n",
    "            cleaned_token.append(single_token)\n",
    "        return cleaned_token\n",
    "\n",
    "    positive = text_cleaner(positive_tweets)\n",
    "    negative = text_cleaner(negative_tweets)\n",
    "\n",
    "    #First split the dataset as the 80% of train, 20% of rest. Then split the rest part into half develop and half test.\n",
    "    pos_train, pos_rest, neg_train, neg_rest = train_test_split(positive, negative, train_size = 0.8)\n",
    "    pos_develop, pos_test, neg_develop, neg_test = train_test_split(pos_rest, neg_rest, train_size = 0.5)\n",
    "\n",
    "    pos_train.extend(neg_train)\n",
    "    pos_develop.extend(neg_develop)\n",
    "    pos_test.extend(neg_test)\n",
    "\n",
    "    train_data = pos_train\n",
    "    develop_data = pos_develop\n",
    "    test_data = pos_test\n",
    "    \n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    def get_BOW(text):\n",
    "        BOW = {}\n",
    "        for word in text:\n",
    "            BOW[word] = BOW.get(word,0) + 1\n",
    "        return BOW\n",
    "\n",
    "    # use the entire dataset to bulid a unified vector, this could ensure the subset feature matrix\n",
    "    # transformed in to same dimension for continues processing\n",
    "    vectorizer = DictVectorizer()\n",
    "    entire_dataset = train_data + develop_data + test_tweets   #change here\n",
    "\n",
    "    vector_shape_unified = []\n",
    "    for i in range (0,len(entire_dataset)):\n",
    "        temp = get_BOW(entire_dataset[i])\n",
    "        vector_shape_unified.append(temp)\n",
    "    unified_set = vectorizer.fit_transform(vector_shape_unified)\n",
    "\n",
    "    def prepare_data(vectorizer,data,feature_extractor):\n",
    "        feature_matrix = []\n",
    "        label = []\n",
    "        for j in range (0,len(data)):\n",
    "            feature_dict = feature_extractor(data[j])\n",
    "            feature_matrix.append(feature_dict)\n",
    "            if j < (len(data)/2):\n",
    "                label.append(\"positive\")\n",
    "            else:\n",
    "                label.append(\"negative\")\n",
    "        dataset = vectorizer.transform(feature_matrix)\n",
    "        return dataset,label\n",
    "\n",
    "    train_set,train_label = prepare_data(vectorizer,train_data,get_BOW)\n",
    "    develop_set,develop_label = prepare_data(vectorizer,develop_data,get_BOW)\n",
    "    #test_set,test_label = prepare_data(vectorizer,test_data,get_BOW)\n",
    "\n",
    "\n",
    "    def prepare_data_test(vectorizer,data,feature_extractor):\n",
    "        feature_matrix = []\n",
    "        for j in range (0,len(data)):\n",
    "            feature_dict = feature_extractor(data[j])\n",
    "            feature_matrix.append(feature_dict)\n",
    "        dataset = vectorizer.transform(feature_matrix)\n",
    "        return dataset\n",
    "\n",
    "    test_tweets_set = prepare_data_test(vectorizer,test_tweets,get_BOW)\n",
    "\n",
    "    NB_best_parameter = best_NB\n",
    "\n",
    "    LR_best_parameter = best_logistic\n",
    "    \n",
    "    NB_classifier = MultinomialNB(alpha = NB_best_parameter, class_prior=None, fit_prior=True)\n",
    "    NB_classifier.fit(train_set,train_label)\n",
    "    NB_predicted = NB_classifier.predict(test_tweets_set)\n",
    "    \n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    for emotion in NB_predicted:\n",
    "        if emotion == 'positive':\n",
    "            positive_count += 1\n",
    "        elif emotion == 'negative':\n",
    "            negative_count += 1\n",
    "    if positive_count > negative_count:\n",
    "        return \"pos\"\n",
    "    else:\n",
    "        return \"neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentiments = defaultdict(int)\n",
    "i = 0\n",
    "for key in user_tweets.keys():\n",
    "    temp = user_tweets[key]\n",
    "    temp_token = text_tokenizer(temp)\n",
    "    test_tweets = text_cleaner(temp_token)\n",
    "    sentiment = sentiment_analysis(test_tweets)\n",
    "    sentiments[sentiment] +=1\n",
    "    print i\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "positive_count = 0\n",
    "negative_count = 0\n",
    "for emotion in NB_predicted:\n",
    "    if emotion == 'positive':\n",
    "        positive_count += 1\n",
    "    elif emotion == 'negative':\n",
    "        negative_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print positive_count\n",
    "print negative_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
